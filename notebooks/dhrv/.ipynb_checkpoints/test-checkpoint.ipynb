{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b53f534-a220-42ac-8a01-cfbbce9469ea",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f71bdf-c875-4564-b9f7-35d4f32eccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import glob\n",
    "import orjson\n",
    "import concurrent.futures\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, JSONLoader, CSVLoader\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import os, glob, concurrent.futures, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283dcc42-c2ad-48bb-9878-a689b3210639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63/1054849735.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_fn = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eecf011197347d7822565ae72e1fff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70ca13d519d4966bf9dd7a103462e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb8e1ab11144b7ebdde4509f9d12aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7474d25e7cf144d1904f2713366908cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be404c56a1949c0989d59bb621ab8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160b026c0add43e894cf9e307154b241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4202043a22d1470795adc7908bb4e1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247763b9a0e04830a850f1b4b7ab434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55d5e4d8d5a4d69a8f838131a022fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d02953d61234358bf343b9371d51e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f180cd55983a4d81812ebb19bf1ec3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Configs\n",
    "\n",
    "DATA_DIR = \"./../../final_train/\"\n",
    "VECTOR_STORE_DIR = \"./../../vector_stores/dhruv_db/\"\n",
    "\n",
    "\n",
    "# Embedding LLM\n",
    "embedding_fn = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # tiny, very fast\n",
    "    model_kwargs={\"device\": \"cuda\"}  # use GPU\n",
    ")\n",
    "\n",
    "# Vector store/datastore #TODO USE FAISS\n",
    "\n",
    "# vector_store = Chroma(\n",
    "#     collection_name=\"example_collection\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=\"./\", #(VECTOR_STORE_DIR + PERSIST_DIR)\n",
    "# )\n",
    "\n",
    "\n",
    "# Document loader\n",
    "pass\n",
    "\n",
    "# Document Splitter\n",
    "pass\n",
    "\n",
    "#Document Indexer\n",
    "pass\n",
    "\n",
    "# Document Retriever\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c9f19-a5f3-4657-a3ad-e7343e90eab3",
   "metadata": {},
   "source": [
    "### Document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953e77bd-a501-407e-b23c-13f0e878f320",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File type: .txt -> 97958 files\n",
      "['./../../final_train/19393327__WQ__E-2008-4260__EN.txt', './../../final_train/22860129__WQA__E-2009-3395__EN.txt', './../../final_train/14701632__QT__H-2007-0537__EN.txt']\n",
      "File type: .json -> 93815 files\n",
      "['./../../final_train/167640-4989_jeopardy_wordphraseorigins_200_881f0e.json', './../../final_train/211850-3224_jeopardy_inventors_100_32f0bd.json', './../../final_train/context_73727.json']\n",
      "File type: .tsv -> 4 files\n",
      "['./../../final_train/collection.tsv', './../../final_train/collection_3dfbf6.tsv', './../../final_train/collection_1.tsv']\n",
      "File type: .tsv# -> 1 files\n",
      "['./../../final_train/.~lock.collection.tsv#']\n"
     ]
    }
   ],
   "source": [
    "def data_loader(data_dir: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Walks through `data_dir` and returns a list of lists of file paths,\n",
    "    grouped by file extension/type.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): The root directory to search in.\n",
    "    \n",
    "    Returns:\n",
    "        List[List[str]]: A list of lists, where each inner list contains\n",
    "                         file paths of a particular file type.\n",
    "    \"\"\"\n",
    "    grouped_files = defaultdict(list)\n",
    "    \n",
    "    # Walk through directory and collect files\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1].lower()  # get extension\n",
    "            file_path = os.path.join(root, file)\n",
    "            grouped_files[ext].append(file_path)\n",
    "    \n",
    "    # Convert to list of lists\n",
    "    return list(grouped_files.values())\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    grouped_paths = data_loader(DATA_DIR)\n",
    "    for group in grouped_paths:\n",
    "        print(f\"File type: {os.path.splitext(group[0])[1]} -> {len(group)} files\")\n",
    "        print(group[:3])  # preview first 3 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f1226-f4bf-492e-8653-4f0f2d05e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Dict, Iterable, List, Sequence, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Community loaders (LangChain >= 0.1.x / 0.2.x)\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    CSVLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredTSVLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    ")\n",
    "\n",
    "# -------- Helper loader functions (each takes a list of file paths) -------- #\n",
    "\n",
    "def load_txt(paths: Sequence[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        # autodetect encoding errors -> replace\n",
    "        docs.extend(TextLoader(p, autodetect_encoding=True, encoding=\"utf-8\").load())\n",
    "    return docs\n",
    "\n",
    "def load_md(paths: Sequence[str]) -> List[Document]:\n",
    "    # Prefer specialized MD loader; TextLoader also works if you don’t need structure\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            docs.extend(UnstructuredMarkdownLoader(p).load())\n",
    "        except Exception:\n",
    "            docs.extend(TextLoader(p, autodetect_encoding=True, encoding=\"utf-8\").load())\n",
    "    return docs\n",
    "\n",
    "def load_csv(paths: Sequence[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        # CSVLoader yields one Document per row by default (metadata includes source)\n",
    "        docs.extend(CSVLoader(p).load())\n",
    "    return docs\n",
    "\n",
    "def load_tsv(paths: Sequence[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        # UnstructuredTSVLoader converts TSV to text blocks\n",
    "        docs.extend(UnstructuredTSVLoader(p).load())\n",
    "    return docs\n",
    "\n",
    "def load_pdf(paths: Sequence[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        docs.extend(PyPDFLoader(p).load())\n",
    "    return docs\n",
    "\n",
    "def load_docx(paths: Sequence[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        docs.extend(Docx2txtLoader(p).load())\n",
    "    return docs\n",
    "\n",
    "def load_html(paths: Sequence[str]) -> List[Document]:\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        docs.extend(UnstructuredHTMLLoader(p).load())\n",
    "    return docs\n",
    "\n",
    "def load_json(paths: Sequence[str], jq_schema: Optional[str] = None, text_content: bool = False) -> List[Document]:\n",
    "    \"\"\"\n",
    "    If you know the JSON shape, pass a jq_schema (e.g., '.items[] | {text: .body, id: .id}').\n",
    "    Otherwise, this will either:\n",
    "      - Use JSONLoader with a simple schema (full object), or\n",
    "      - Fallback to raw text via TextLoader if JSONLoader fails.\n",
    "    \"\"\"\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if jq_schema:\n",
    "                docs.extend(JSONLoader(p, jq_schema=jq_schema, text_content=text_content).load())\n",
    "            else:\n",
    "                # Use whole JSON object as a single doc string\n",
    "                docs.extend(JSONLoader(p, jq_schema=\".\", text_content=True).load())\n",
    "        except Exception:\n",
    "            # Fallback: just read raw JSON text\n",
    "            docs.extend(TextLoader(p, autodetect_encoding=True, encoding=\"utf-8\").load())\n",
    "    return docs\n",
    "\n",
    "\n",
    "# -------- Extension → helper mapping -------- #\n",
    "\n",
    "LoaderFn = Callable[[Sequence[str]], List[Document]]\n",
    "\n",
    "DEFAULT_LOADERS: Dict[str, LoaderFn] = {\n",
    "    \".txt\": load_txt,\n",
    "    \".md\": load_md,\n",
    "    \".markdown\": load_md,\n",
    "    \".csv\": load_csv,\n",
    "    \".tsv\": load_tsv,\n",
    "    \".pdf\": load_pdf,\n",
    "    \".docx\": load_docx,\n",
    "    \".html\": load_html,\n",
    "    \".htm\": load_html,\n",
    "    \".json\": load_json,   # uses default behavior; see override pattern below\n",
    "    # add more specialized handlers as needed\n",
    "}\n",
    "\n",
    "FALLBACK_LOADER: LoaderFn = load_txt  # If unknown extension, treat as text\n",
    "\n",
    "\n",
    "# -------- Main API -------- #\n",
    "\n",
    "def build_langchain_docs(\n",
    "    grouped_paths: List[List[str]],\n",
    "    loader_map: Optional[Dict[str, LoaderFn]] = None,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Convert output of data_loader (List[List[str]]) into LangChain Documents\n",
    "    using the appropriate loader per file type.\n",
    "\n",
    "    Assumptions:\n",
    "      - Each inner list is homogeneous (all same extension).\n",
    "      - We infer the extension from the first path in each group.\n",
    "\n",
    "    Args:\n",
    "        grouped_paths: Output of your data_loader (list of lists of file paths).\n",
    "        loader_map: Optional override / extension of DEFAULT_LOADERS.\n",
    "                    Keys are lowercase extensions like \".json\", \".pdf\", etc.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: All loaded documents (no splitting/indexing).\n",
    "    \"\"\"\n",
    "    if loader_map is None:\n",
    "        loader_map = dict(DEFAULT_LOADERS)\n",
    "\n",
    "    all_docs: List[Document] = []\n",
    "    for group in grouped_paths:\n",
    "        if not group:\n",
    "            continue\n",
    "        ext = os.path.splitext(group[0])[1].lower()  # infer from the first file\n",
    "        loader: LoaderFn = loader_map.get(ext, FALLBACK_LOADER)\n",
    "        docs = loader(group)\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# -------- Example: customizing JSON handling (optional) -------- #\n",
    "# If your JSON is an array of objects with fields { \"id\": ..., \"body\": ... } and you\n",
    "# want each object as a Document with text=body and metadata including id:\n",
    "#\n",
    "# custom_loader_map = dict(DEFAULT_LOADERS)\n",
    "# custom_loader_map[\".json\"] = lambda paths: load_json(\n",
    "#     paths,\n",
    "#     jq_schema='.[] | {id: .id, text: .body}',\n",
    "#     text_content=True,   # put .text into Document.page_content\n",
    "# )\n",
    "# docs = build_langchain_docs(grouped_paths, loader_map=custom_loader_map)\n",
    "\n",
    "docs = build_langchain_docs(grouped_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9e156-9523-4e14-b205-4b65805ecd3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11315f97-8fcb-4b7b-85a7-dde13b63815c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acf3fec6-3aa5-4806-8367-42ba2252eae5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Document Indexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773194c-6116-4254-b936-cba6d0c5b55e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Document Retriever\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
