{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc1f319-2f87-47f0-b822-0ddd3c4c6b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/root/.cache/uv/builds-v0/.tmpOVd3UX` does not match the project environment path `/app/.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m229 packages\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mAudited \u001b[1m222 packages\u001b[0m \u001b[2min 0.02ms\u001b[0m\u001b[0m                                       \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add langchain langchain-huggingface langchain-chroma chromadb tiktoken ijson pandas torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c948f98-7457-4aaa-883a-cb74581ef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from glob import glob\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, JSONLoader, CSVLoader\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd5ba4b-f0b6-43ec-88cc-edd135552fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./../final_train/\"\n",
    "VECTOR_DB = \"./chroma_db\"   # local folder, not /app (unless inside Docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae1f2913-286c-4528-a369-3807ec82d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54406065-000e-4e90-840a-09d4191c697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    persist_directory=VECTOR_DB,\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026b1d7f-277c-4d1a-9a7b-e9f39c0db7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a61a3676-84f9-4343-a678-73ae8db1c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_add_files(file_paths):\n",
    "    for path in file_paths:\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext == \".txt\":\n",
    "            docs = load_txt(path)\n",
    "        elif ext == \".tsv\":\n",
    "            docs = load_tsv(path)\n",
    "        elif ext == \".json\":\n",
    "            docs = load_custom_json(path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping unsupported file: {path}\")\n",
    "            continue\n",
    "\n",
    "            \n",
    "        split_docs = splitter.split_documents(docs)\n",
    "        vector_store.add_documents(split_docs)\n",
    "        print(f\"üìö Indexed {len(split_docs)} chunks from {os.path.basename(path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f77fa74b-c952-4cd5-b91a-b7808e0e5b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m split_docs = splitter.split_documents(\u001b[43mdocs\u001b[49m)\n\u001b[32m      2\u001b[39m vector_store.add_documents(split_docs)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìö Indexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(split_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.basename(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "split_docs = splitter.split_documents(docs)\n",
    "vector_store.add_documents(split_docs)\n",
    "print(f\"üìö Indexed {len(split_docs)} chunks from {os.path.basename(path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d60195f-9d57-4a5f-952e-397b2525059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT: 97958 JSON: 27366 TSV: 4\n"
     ]
    }
   ],
   "source": [
    "txt_files = glob(os.path.join(DATA_DIR, \"**\", \"*.txt\"), recursive=True)\n",
    "json_files = glob(os.path.join(DATA_DIR, \"**\", \"context_*.json\"), recursive=True)\n",
    "tsv_files = glob(os.path.join(DATA_DIR, \"**\", \"*.tsv\"), recursive=True)\n",
    "\n",
    "print(\"TXT:\", len(txt_files), \"JSON:\", len(json_files), \"TSV:\", len(tsv_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffcbdb2b-fec0-4cb3-aef5-60213a943a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(path):\n",
    "    return TextLoader(path, encoding=\"utf-8\").load()\n",
    "\n",
    "def load_json(path):\n",
    "    # Assumes JSON is an array of objects, each with a \"text\" key\n",
    "    return JSONLoader(path, jq_schema=\".[]\", text_content_key=\"text\").load()\n",
    "\n",
    "def load_tsv(path):\n",
    "    return CSVLoader(path, encoding=\"utf-8\", csv_args={\"delimiter\": \"\\t\"}).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "287e9908-51a6-42d5-8454-025a6bd77160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Type of JSON root: <class 'dict'>\n",
      "First few keys:\n",
      " {\n",
      "  \"context\": [\n",
      "    [\n",
      "      \"Frailea\",\n",
      "      [\n",
      "        \"Frailea is a genus of globular to short cylindrical cacti native to Brazil.\",\n",
      "        \" These species are cleistogamous.\",\n",
      "        \" They were first classified in the genus \\\"Echinocactus\\\".\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Hoodia alstonii\",\n",
      "      [\n",
      "        \"Hoodia alstonii is a succulent plant native to Namibia and the Cape Province of South Africa.\",\n",
      "        \" \\\"H. alstonii\\\" is also known commonly as Ghaap, an Afrikaans name.\",\n",
      "        \" It tends to grow in rocky, desert areas.\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Hawaii Route 92\",\n",
      "      [\n",
      "        \"Route 92 is a major east\\u2013west highway on the island of Oahu which begins at exit 15 off Interstate H-1 in Honolulu and ends 0.6 mi east of the Ala Wai Canal crossing in Waikiki.\",\n",
      "        \" The western portion, west of Richards Street, is locally known as the Nimitz Highway (named after Pacific Fleet Admiral during World War II, Chester Nimitz).\",\n",
      "        \" East of Richards Street, Route 92 is locally known as Ala Moana Boulevard.\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Hoodia gordonii\",\n",
      "      [\n",
      "        \"Hoodia gordonii, also known as Bushman's hat, is a leafless spiny succulent plant supposed to have therapeutic properties in folk medicine.\",\n",
      "        \" It grows naturally in Botswana, South Africa and Namibia.\",\n",
      "        \" The species became internationally known and threatened by collectors, after a marketing campaign falsely claimed that it was an appetite suppressant for weight loss.\",\n",
      "        \" The flowers smell like rotten meat and are pollinated mainly by flies.\",\n",
      "        \" The indigenous San people (Bushmen) of the Namib desert call this plant \\u01c1hoba (pronounced \\u2013 the initial sound is a lateral click) \\u2013 and the Afrikaans name ghaap is used to refer to all species of \\\"Hoodia\\\".\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Hoodia flava\",\n",
      "      [\n",
      "        \"Hoodia flava is a succulent native to the Cape Province in South Africa and to Namibia.\",\n",
      "        \" It has a unique pattern of distribution, growing inside bushes or on gravelly slopes and hills.\",\n",
      "        \" It is commonly known as Ghaap or Yellow-flowered Ghaap in the Afrikaans language.\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Donax trunculus\",\n",
      "      [\n",
      "        \"Donax trunculus (abrupt wedge shell, wedge clam), is a bivalve species in the family Donacidae.\",\n",
      "        \" It is native to the Mediterranean and Atlantic coasts of western Europe.\",\n",
      "        \" It is locally known as \\\"tellin\\\", \\\"tellina\\\", \\\"telline\\\" or \\\"tenille\\\" in French, \\\"tellina\\\" or \\\"coquina\\\" in Spanish, and \\\"conquilha\\\" or \\\"cadelinha\\\" in Portuguese, and is consumed as a food in these countries.\",\n",
      "        \" There is a very similar shellfish in Australia locally known as \\\"Pippies\\\"\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Missouri Route 267\",\n",
      "      [\n",
      "        \"Route 267 is a short state highway in the St. Louis, Missouri area.\",\n",
      "        \" Its northern terminus is at Broadway in south St. Louis; its southern terminus is at an intersection with U.S. Routes 50, 61, and U.S. Route 67, locally known as Lindbergh Boulevard (to the east and west) and Lemay Ferry Road to the south.\",\n",
      "        \" The route is locally known as Lemay Ferry Road.\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Tennessee State Route 268\",\n",
      "      [\n",
      "        \"State Route 268 (SR 268) is a secondary state highway in Rutherford County and Murfreesboro, Tennessee, USA.\",\n",
      "        \" It runs from US 41/70S (locally known as North West Broad Street) on the north west side of Murfreesboro, to its Eastern terminus with SR-96 (locally known as Lascassas Pike), north-east of Murfreesboro.\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Hoodia currorii\",\n",
      "      [\n",
      "        \"Hoodia currorii is a succulent plant native to Namibia and the Cape Province of South Africa.\",\n",
      "        \" It grows in desert areas and is common along the road from Karibib to Swakopmund in Namibia.\",\n",
      "        \" It is also known as Ghaap in the vernacular.\"\n",
      "      ]\n",
      "    ],\n",
      "    [\n",
      "      \"Hoodia\",\n",
      "      [\n",
      "        \"Hoodia (known locally as \\\"ghaap\\\" or \\\"bobbejaanghaap\\\") is a genus of flowering plants in the family Apocynaceae, under the subfamily Asclepiadoideae, native to Southern Africa.\"\n",
      "      ]\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(json_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"üîé Type of JSON root:\", type(data))\n",
    "if isinstance(data, list):\n",
    "    print(\"First 2 entries:\\n\", json.dumps(data[:2], indent=2))\n",
    "elif isinstance(data, dict):\n",
    "    # Print first 3 keys only\n",
    "    keys = list(data.keys())[:3]\n",
    "    preview = {k: data[k] for k in keys}\n",
    "    print(\"First few keys:\\n\", json.dumps(preview, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea7b8afa-1c7a-4746-bd09-c801738e4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_dataset(batch_size: int = 500):\n",
    "    files = glob(os.path.join(DATA_DIR, \"*\"))\n",
    "    total_docs = 0\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            buffer = []\n",
    "            for doc in stream_json(file):\n",
    "                buffer.append(doc)\n",
    "                if len(buffer) >= batch_size:\n",
    "                    vectorstore.add_documents(buffer)\n",
    "                    total_docs += len(buffer)\n",
    "                    print(f\"Indexed {total_docs} docs so far from {file}...\")\n",
    "                    buffer = []\n",
    "            if buffer:\n",
    "                vectorstore.add_documents(buffer)\n",
    "                total_docs += len(buffer)\n",
    "\n",
    "        elif file.endswith(\".tsv\"):\n",
    "            buffer = []\n",
    "            for doc in load_tsv(file):\n",
    "                buffer.append(doc)\n",
    "                if len(buffer) >= batch_size:\n",
    "                    vectorstore.add_documents(buffer)\n",
    "                    total_docs += len(buffer)\n",
    "                    print(f\"Indexed {total_docs} docs so far from {file}...\")\n",
    "                    buffer = []\n",
    "            if buffer:\n",
    "                vectorstore.add_documents(buffer)\n",
    "                total_docs += len(buffer)\n",
    "\n",
    "        elif file.endswith(\".txt\"):\n",
    "            docs = list(load_txt(file))\n",
    "            vectorstore.add_documents(docs)\n",
    "            total_docs += len(docs)\n",
    "            print(f\"Indexed {len(docs)} docs from {file}\")\n",
    "\n",
    "    if total_docs > 0:\n",
    "        vectorstore.persist()\n",
    "        print(f\"‚úÖ Finished ingestion: {total_docs} total documents stored in Chroma\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No documents ingested. Check dataset format.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521a102-2b8e-4062-9589-faeed768cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"What does the dataset say about object detection?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\\n{doc.page_content[:500]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d768a2c-3fa1-4d64-a3b4-9c1125131deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "DATA_DIR = \"../final_train\"  # adjust if wrong\n",
    "files = glob(os.path.join(DATA_DIR, \"*\"))\n",
    "print(\"Files found:\", files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280af7-c52f-431f-b575-b46a9db53904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5db562-f57f-4641-98c0-c260010e8ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
