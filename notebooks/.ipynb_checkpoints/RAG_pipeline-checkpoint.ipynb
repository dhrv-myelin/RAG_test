{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3d8b9f-d619-41ce-85bc-7e7b7ce9fd27",
   "metadata": {},
   "source": [
    "# RAG PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a880eb7-1ef4-4772-8784-c08f630aa012",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3997f-923f-4491-b558-14ef911a4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run only once\n",
    "!uv add langchain-text-splitters langchain-community langgraph\n",
    "!uv add \"langchain-perplexity\"\n",
    "!uv add langchain-ollama\n",
    "!uv add chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248c0397-d074-4b2f-98c0-1f304bf997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader, JSONLoader, CSVLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72a084-b90a-419e-af81-159873e58d06",
   "metadata": {},
   "source": [
    "### generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afeb19f-831d-41e1-a0a0-b5b60f56f9c7",
   "metadata": {},
   "source": [
    "### embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e0c5dc-cc5c-43e9-a7cd-7e90381203ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pulling ollama embedding model\n",
    "\n",
    "# OLLAMA_API = \"http://ollama:11434/api/pull\"\n",
    "# model_name = \"llama3\"\n",
    "\n",
    "# with requests.post(OLLAMA_API, json={\"name\": model_name}, stream=True) as r:\n",
    "#     for line in r.iter_lines():\n",
    "#         if line:\n",
    "#             data = json.loads(line.decode(\"utf-8\"))\n",
    "#             print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ff2be-815f-42b0-9b51-8b06b486e231",
   "metadata": {},
   "source": [
    "## Embeddings and vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132c89b4-632a-4d69-8c94-f1bd68edcc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3\",\n",
    "    base_url=\"http://ollama:11434\"  # service name from docker-compose\n",
    ")\n",
    "\n",
    "# Chroma will store data locally in /app/chroma_db\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"/app/chroma_db\"  # relative to /app in container\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8795a-4c16-4799-adba-d2088ad2d60d",
   "metadata": {},
   "source": [
    "## Document loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "324e6895-dc0b-45ef-95aa-96a1f44c6fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./../final_train/context_73727.json']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "JSONLoader.__init__() got an unexpected keyword argument 'text_content_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m json_file = json_files[:\u001b[32m1\u001b[39m]\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Run bulk processing\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# process_and_add_files(txt_files, load_txt)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mprocess_and_add_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# process_and_add_files(tsv_files, load_tsv)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mprocess_and_add_files\u001b[39m\u001b[34m(file_paths, loader_fn)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_and_add_files\u001b[39m(file_paths, loader_fn):\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         docs = \u001b[43mloader_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28mprint\u001b[39m(docs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mload_json\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json\u001b[39m(path):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJSONLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjq_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.[]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_content_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.load()\n",
      "\u001b[31mTypeError\u001b[39m: JSONLoader.__init__() got an unexpected keyword argument 'text_content_key'"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "DATA_DIR = \"./../final_train/\"\n",
    "VECTOR_DB = \"/app/chroma_db/\"\n",
    "\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# Loaders\n",
    "def load_txt(path):\n",
    "    return TextLoader(path, encoding='utf-8').load()\n",
    "def load_json(path):\n",
    "    return JSONLoader(path, jq_schema='.[]', text_content_key='text').load()\n",
    "def load_tsv(path):\n",
    "    return CSVLoader(path, encoding='utf-8', csv_args={'delimiter': '\\t'}).load()\n",
    "\n",
    "# Automated dispatcher\n",
    "def process_and_add_files(file_paths, loader_fn):\n",
    "    \n",
    "    for path in file_paths:\n",
    "        docs = loader_fn(path)\n",
    "        print(docs)\n",
    "        #split docs\n",
    "        split_docs = splitter.split_documents(docs)\n",
    "        vector_store.add_documents(split_docs)\n",
    "        print(f\"Indexed {len(split_docs)} chunks from {path}\")\n",
    "\n",
    "# Recursively collect files\n",
    "txt_files = glob(os.path.join(DATA_DIR, \"**\", \"*.txt\"), recursive=True)\n",
    "# json_files = glob(os.path.join(DATA_DIR, \"**\", \"*.json\"), recursive=True)\n",
    "json_files = glob(os.path.join(DATA_DIR, \"**\", \"context_*.json\"), recursive=True)\n",
    "tsv_files = glob(os.path.join(DATA_DIR, \"**\", \"*.tsv\"), recursive=True)\n",
    "\n",
    "# print(txt_files[:5])\n",
    "print(json_files[:1])\n",
    "# print(tsv_files[:5])\n",
    "\n",
    "\n",
    "with open(json_files[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    print(json.dumps(data, indent=2))  # pretty print\n",
    "# Run bulk processing\n",
    "# process_and_add_files(txt_files, load_txt)\n",
    "# process_and_add_files(json_files, load_json)\n",
    "# process_and_add_files(tsv_files, load_tsv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e97915-5165-4e1c-a3f8-aae5a2f943db",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d88f54-3ebb-4eff-a0c9-65cadbb8ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, chromadb_vectorstore, embeddings, k=4):\n",
    "    return chromadb_vectorstore.similarity_search(\n",
    "        query,\n",
    "        embeddings=embeddings,\n",
    "        k=k  # number of results\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "query = \"Explain Task Decomposition\"\n",
    "retrieved_docs = retrieve_documents(query, chromadb_vectorstore, embeddings, k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6b402-f0bc-4d48-8202-935f01655014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_documents(docs):\n",
    "    for idx, doc in enumerate(docs, 1):\n",
    "        print(f\"--- Document {idx} ---\")\n",
    "        print(doc.page_content[:500])  # Print only the start for brevity\n",
    "        print()\n",
    "\n",
    "# Usage\n",
    "print_documents(retrieved_docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
